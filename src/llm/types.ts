
/**
 * @file Defines core abstract types and interfaces for LLM interactions.
 * This abstraction allows the system to be largely independent of specific LLM providers.
 */

import { IToolDefinition, IToolResult } from '../core/tool';

/**
 * Represents the role of a message in an LLM conversation.
 */
export type LLMMessageRole = 'system' | 'user' | 'assistant' | 'tool';

/**
 * Represents a single tool call made by an LLM.
 */
export interface LLMToolCall {
  /** A unique identifier for this specific tool call, generated by the LLM. */
  id: string;
  /** The type of the tool call, typically "function". */
  type: 'function'; // Currently, only 'function' type is widely supported.
  /** Details of the function to be called. */
  function: {
    /** The name of the function/tool to call. */
    name: string;
    /**
     * The arguments to pass to the function, as a JSON string.
     * The LLM is expected to generate a string that can be parsed into a JSON object.
     */
    arguments: string;
  };
}

/**
 * Represents a message in a conversation with an LLM.
 * This interface is designed to be compatible with common LLM API structures (e.g., OpenAI).
 */
export interface LLMMessage {
  /** The role of the entity creating the message. */
  role: LLMMessageRole;
  /**
   * The content of the message.
   * Can be a simple string or an array for multi-part messages (e.g., text + image URLs).
   * For 'tool' role, this contains the result of the tool execution.
   */
  content:
    | string
    | Array<
        | { type: 'text'; text: string }
        | { type: 'image_url'; image_url: { url: string; detail?: 'low' | 'high' | 'auto' } }
      >;
  /**
   * Optional: The name of the tool/function, used when `role` is 'tool' (for the result)
   * or 'assistant' (if the assistant is directly calling a function in some legacy formats, though `tool_calls` is preferred).
   */
  name?: string;
  /**
   * Optional: A list of tool calls requested by the assistant.
   * This is used when `role` is 'assistant' and the LLM decides to call one or more tools.
   */
  tool_calls?: LLMToolCall[];
  /**
   * Optional: The ID of the tool call this message is a response to.
   * This is used when `role` is 'tool'.
   */
  tool_call_id?: string;
}

/**
 * Represents a chunk of a streaming LLM message.
 * Fields are optional as not all parts of a message arrive in every chunk.
 */
export interface LLMMessageChunk {
  role?: LLMMessageRole;
  content?: string | null; // Content can be null in some chunks, or an empty string.
  name?: string;
  tool_calls?: Array<{
    index: number; // Index of the tool call in the tool_calls array
    id?: string;
    type?: 'function';
    function?: {
      name?: string;
      arguments?: string; // Argument parts arrive as string chunks
    };
  }>;
  tool_call_id?: string;
  /** Optional: The reason the stream or generation finished (e.g., 'stop', 'length', 'tool_calls'). */
  finish_reason?: string | null;
  /** Optional: Usage statistics, often provided at the end of a stream or in a non-streaming response. */
  usage?: {
    prompt_tokens?: number;
    completion_tokens?: number;
    total_tokens?: number;
  };
}

/**
 * Defines how the LLM should choose which tool to call, if any.
 * - `auto` (default): LLM decides whether to call a tool or generate a text response.
 * - `none`: LLM is forced to generate a text response, cannot call tools.
 * - `required`: LLM is forced to call at least one tool.
 * - `{ type: "function", function: { name: "my_tool_name" } }`: LLM is forced to call a specific tool.
 */
export type LLMToolChoice = 'auto' | 'none' | 'required' | { type: 'function'; function: { name: string } };

/**
 * Interface for an LLM client, abstracting specific SDKs or APIs.
 */
export interface ILLMClient {
  /**
   * Generates a response from the LLM based on a list of messages.
   * Can handle both streaming and non-streaming requests.
   *
   * @param messages A list of `LLMMessage` objects representing the conversation history.
   * @param options Optional parameters for the LLM call.
   * @param options.model The model to use for generation.
   * @param options.tools Optional list of tool definitions (in the LLM provider's specific format) available for the LLM to call.
   * @param options.tool_choice Optional: Controls how the LLM uses tools.
   * @param options.stream Whether to stream the response. If true, returns an AsyncGenerator.
   * @param options.temperature Sampling temperature.
   * @param options.max_tokens Maximum number of tokens to generate.
   * @param options.systemPrompt Optional override or augmentation for the system prompt if the adapter supports it.
   * @param otherOptions Additional provider-specific options.
   * @returns A Promise resolving to an `LLMMessage` (for non-streaming) or an `AsyncGenerator` yielding `LLMMessageChunk` (for streaming).
   * @throws `LLMError` if the API call fails.
   */
  generateResponse(
    messages: LLMMessage[],
    options: {
      model: string;
      tools?: any[]; // This will be the LLM provider's specific tool format
      tool_choice?: LLMToolChoice;
      stream?: boolean;
      temperature?: number;
      max_tokens?: number;
      systemPrompt?: string; // Some adapters might allow a direct system prompt override here
      [otherOptions: string]: any; // For additional provider-specific parameters
    }
  ): Promise<LLMMessage | AsyncGenerator<LLMMessageChunk, void, unknown>>;

  /**
   * Counts the number of tokens for a given list of messages and model.
   *
   * @param messages A list of `LLMMessage` objects.
   * @param model The model for which to count tokens.
   * @returns A Promise resolving to the token count.
   * @throws `ConfigurationError` if token counting is not supported for the model or `LLMError` for API issues.
   */
  countTokens(messages: LLMMessage[], model: string): Promise<number>;

  formatToolsForProvider(toolDefinitions: IToolDefinition[]): any[]; // Returns provider-specific tool format
}

/**
 * Describes the structure of a tool function specifically for LLMs like OpenAI,
 * which expect a single JSON schema object for all parameters.
 * This is the target format for adapters like `openai-tool-adapter.ts`.
 * (This was previously in `src/llm/types.ts` and remains relevant here).
 */
export interface LLMToolFunctionDefinition {
  name: string;
  description: string;
  /**
   * A single JSON Schema object describing the parameters of the function.
   * The top level should be type: 'object', with 'properties' detailing each parameter,
   * and an optional 'required' array.
   */
  parametersSchema: {
    type: 'object';
    properties: Record<string, any>; // Each property is a JSON schema for a parameter
    required?: string[];
  };
}



// src/llm/types.ts

/**
 * @file Defines types specific to LLM interactions, particularly for tool definitions
 *       in formats expected by certain LLM providers like OpenAI.
 */

/**
 * Describes the structure of a tool function specifically for LLMs like OpenAI,
 * which typically expect a single, consolidated JSON schema object for all parameters.
 */
// export interface LLMToolFunctionDefinition {
//   /**
//    * The name of the tool. Must be a-z, A-Z, 0-9, or contain underscores and dashes,
//    * with a maximum length of 64 characters (OpenAI requirement).
//    */
//   name: string;
//   /**
//    * A description of what the function does, used by the model to choose when and
//    * how to call the function.
//    */
//   description: string;
//   /**
//    * The parameters the functions accepts, described as a JSON Schema object.
//    * The top level of this schema MUST be of type 'object'.
//    * @see https://platform.openai.com/docs/guides/function-calling
//    * @example
//    * {
//    *   type: 'object',
//    *   properties: {
//    *     location: { type: 'string', description: 'The city and state, e.g. San Francisco, CA' },
//    *     unit: { type: 'string', enum: ['celsius', 'fahrenheit'] }
//    *   },
//    *   required: ['location']
//    * }
//    */
//   parametersSchema: {
//     type: 'object';
//     properties: Record<string, any>; // Each property is a JSON schema definition for a parameter
//     required?: string[];
//   };
// }

// Additional LLM-specific types can be added here as the project evolves.
// For example:
// - Types for LLM chat message structures (user, assistant, system, tool/function call, tool/function result).
// - Types for specific LLM provider responses.
